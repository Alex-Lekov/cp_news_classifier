{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Then what you need from tensorflow.keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertForSequenceClassification, \\\n",
    "    DistilBertConfig, DistilBertTokenizerFast\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers package version: 4.17.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(f\"Transformers package version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LENGTH = 50  # We truncate anything after the 200-th word to speed up training\n",
    "TEGS = [\n",
    "    'release_points', \n",
    "    'technical_update_points',\n",
    "    'partnership_points', \n",
    "    'listing_points', \n",
    "    'security_points',\n",
    "    'from_the_project', \n",
    "    'not_from_the_project', \n",
    "    'staking'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_accuracy(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"For multi-label classification, one has to define a custom\n",
    "    acccuracy function because neither tf.keras.metrics.Accuracy nor\n",
    "    tf.keras.metrics.CategoricalAccuracy evaluate the number of \n",
    "    exact matches.\n",
    "\n",
    "    :Example:\n",
    "    >>> from tensorflow.keras import metrics\n",
    "    >>> y_true = tf.convert_to_tensor([[1., 1.]])\n",
    "    >>> y_pred = tf.convert_to_tensor([[1., 0.]])\n",
    "    >>> metrics.Accuracy()(y_true, y_pred).numpy()\n",
    "    0.5\n",
    "    >>> metrics.CategoricalAccuracy()(y_true, y_pred).numpy()\n",
    "    1.0\n",
    "    >>> multi_label_accuracy(y_true, y_pred).numpy()\n",
    "    0.0\n",
    "    \"\"\"   \n",
    "    y_pred = tf.math.round(y_pred)\n",
    "    exact_matches = tf.math.reduce_all(y_pred == y_true, axis=1)\n",
    "    exact_matches = tf.cast(exact_matches, tf.float32)\n",
    "    return tf.math.reduce_mean(exact_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"We'd like to remind everyone that our $BONDLY token contract remains compromised by an unknown attacker and we ask you to refrain from trading our token until we have redeployed our new token. \\n\\nMore details here: https://t.co/WuSSNt2bsH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_2 = \"@LuckyBartlett We'll be releasing details soon, including for those who held LP tokens, apologies for the delay\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 18:27:24.426711: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-03-30 18:27:25.275106: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-30 18:27:25.275128: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-30 18:27:25.275139: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Ubuntu-2004-focal-64-minimal): /proc/driver/nvidia/version does not exist\n",
      "2022-03-30 18:27:25.275260: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "config = DistilBertConfig.from_pretrained(MODEL_NAME)\n",
    "model = load_model(\"model.h5\",\n",
    "    custom_objects={\n",
    "        \"multi_label_accuracy\": multi_label_accuracy,\n",
    "        \"RectifiedAdam\": tfa.optimizers.RectifiedAdam\n",
    "        }\n",
    "    )\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_text(text, model=model, tokenizer=tokenizer):\n",
    "    padded_encodings = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=MAX_LENGTH, # truncates if len(s) > max_length\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return model(padded_encodings[\"input_ids\"]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00151449, 0.00014731, 0.00303701, 0.00015244, 0.00022259,\n",
       "        0.00300139, 0.00018477, 0.00069699]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_text(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False,  True,  True, False, False]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(score_text(sample_text) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_text(sample_text_2) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['security_points', 'from_the_project']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import compress\n",
    "\n",
    "text_tegs = list(compress(TEGS, (score_text(sample_text) > 0.5)[0]))\n",
    "text_tegs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"MODEL_NAME\": MODEL_NAME,\n",
    "    \"MAX_LENGTH\": MAX_LENGTH,\n",
    "    \"TEGS\": TEGS,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'w') as file:\n",
    "    yaml.dump(config, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAX_LENGTH': 50, 'MODEL_NAME': 'distilbert-base-uncased', 'TEGS': ['release_points', 'technical_update_points', 'partnership_points', 'listing_points', 'security_points', 'from_the_project', 'not_from_the_project', 'staking']}\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.yaml\", 'r') as stream:\n",
    "    try:\n",
    "        config=yaml.safe_load(stream)\n",
    "        print(config)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1513efd8ec93a19fa8591704325549dd1c7568c2987ef819f7a3c56ad8655b40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('kattana_p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
